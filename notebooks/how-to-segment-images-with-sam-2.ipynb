{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Segment Images with Segment Anything 2 (SAM2)\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything-2)\n",
        "\n",
        "Segment Anything Model 2 (SAM 2) is a foundation model designed to address promptable visual segmentation in both images and videos. The model extends its functionality to video by treating images as single-frame videos. Its design, a simple transformer architecture with streaming memory, enables real-time video processing. A model-in-the-loop data engine, which enhances the model and data through user interaction, was built to collect the SA-V dataset, the largest video segmentation dataset to date. SAM 2, trained on this extensive dataset, delivers robust performance across diverse tasks and visual domains.\n",
        "\n",
        "![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-2-paper.jpg)\n",
        "\n",
        "This notebook is an extension of the official [notebook](https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb) prepared by Meta AI.\n",
        "\n",
        "## Complementary materials\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb)\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-segment-anything-2)\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on Segment Anything Model 2 (SAM2).\n",
        "\n",
        "[![SAM2 blogpost](https://media.roboflow.com/notebooks/examples/blog-what-is-sam-2.png)](https://blog.roboflow.com/what-is-segment-anything-2)"
      ],
      "metadata": {
        "id": "l2Es_L1iNX4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "8wkV75Db9tXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "DSffnnWDNhb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA9bQMozM_wg",
        "outputId": "255d105c-ad72-4b15-ad44-83537a4a70ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "H7YQbFlINnGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx8UNmABNkJP",
        "outputId": "3e89df95-3178-4834-f19b-5eb56f9d5c49"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content/segment-anything-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SAM2 and dependencies"
      ],
      "metadata": {
        "id": "yo5LAKqyNzfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBrYxp6nNpqk",
        "outputId": "8657ae87-cb13-408e-9508-67cba22fba26"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'segment-anything-2'...\n",
            "remote: Enumerating objects: 1070, done.\u001b[K\n",
            "remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1070/1070), 128.11 MiB | 26.46 MiB/s, done.\n",
            "Resolving deltas: 100% (381/381), done.\n",
            "/content/segment-anything-2/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision jupyter_bbox_widget"
      ],
      "metadata": {
        "id": "UGv98ypgPA7c"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download SAM2 checkpoints\n",
        "\n",
        "**NOTE:** SAM2 is available in 4 different model sizes ranging from the lightweight \"sam2_hiera_tiny\" (38.9M parameters) to the more powerful \"sam2_hiera_large\" (224.4M parameters)."
      ],
      "metadata": {
        "id": "g3Psmg3sOzIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints"
      ],
      "metadata": {
        "id": "Dq_DR0IJN_1H"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download example data\n",
        "\n",
        "**NONE:** Let's download few example images. Feel free to use your images or videos."
      ],
      "metadata": {
        "id": "7zPgcOxiQHiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg -P {HOME}/data\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg -P {HOME}/data"
      ],
      "metadata": {
        "id": "_p1zpkKZPvFu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "BHRLQPV4WKd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
      ],
      "metadata": {
        "id": "vIcNq3IiXufS"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance."
      ],
      "metadata": {
        "id": "svThmVIGZZAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLeXwS2UQU5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)"
      ],
      "metadata": {
        "id": "xHvgsf08QRZo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_generator = SAM2AutomaticMaskGenerator(sam2_model)"
      ],
      "metadata": {
        "id": "72oiBQYvUSws"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** OpenCV loads images in BGR format by default, so we convert to RGB for compatibility with the mask generator."
      ],
      "metadata": {
        "id": "wHrJV4HmavcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting with boxes"
      ],
      "metadata": {
        "id": "ijdVA3p0cyJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = SAM2ImagePredictor(sam2_model)"
      ],
      "metadata": {
        "id": "gZdNS8fJZ1B4"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = f\"{HOME}/MR/MR.jpg\"\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "HZTCMn0MeO7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "6a852bfd-cc21-4dfb-d3e1-740fcc8e139d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-4364018e5313>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage_bgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÂàáÊç¢‰∏ã‰∏ÄÂº†ÂõæÁâá„ÄÇ"
      ],
      "metadata": {
        "id": "AfFtAGHP4RUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "folder_path = f\"{HOME}/data\"\n",
        "\n",
        "current_image_name = os.path.basename(IMAGE_PATH)\n",
        "\n",
        "# Ëé∑ÂèñÊéíÂ∫èÂêéÁöÑÂõæÂÉèÂàóË°®\n",
        "image_list = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "# Ëé∑ÂèñÂΩìÂâçÂõæÂÉèÁ¥¢ÂºïÂπ∂Êâæ‰∏ã‰∏Ä‰∏™ÂõæÂÉè\n",
        "current_index = image_list.index(current_image_name)\n",
        "if current_index + 1 < len(image_list):\n",
        "    next_image_name = image_list[current_index + 1]\n",
        "    IMAGE_PATH = os.path.join(folder_path, next_image_name)\n",
        "else:\n",
        "    print(\"No more images.\")\n",
        "    IMAGE_PATH = None\n",
        "\n",
        "# ËØªÂèñÂπ∂Â§ÑÁêÜÂõæÂÉè\n",
        "if IMAGE_PATH:\n",
        "    image_bgr = cv2.imread(IMAGE_PATH)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "print(IMAGE_PATH)"
      ],
      "metadata": {
        "id": "r8WA-OQgwBpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interactive box prompt"
      ],
      "metadata": {
        "id": "MMzeqSsgejYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded"
      ],
      "metadata": {
        "id": "VNWUblWreTW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Execute cell below and use your mouse to **draw bounding box** on the image üëá"
      ],
      "metadata": {
        "id": "A9skH4bceoWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "gNwusrHEek_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widget.bboxes"
      ],
      "metadata": {
        "id": "4SFmFWugeqR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** `Sam2ImagePredictor.predict` method takes `np.ndarray` `box` argument in `[x_min, y_min, x_max, y_max]` format."
      ],
      "metadata": {
        "id": "G4vNm8trfN10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 166, 'y': 835, 'width': 99, 'height': 175, 'label': ''},\n",
        "    {'x': 472, 'y': 885, 'width': 168, 'height': 249, 'label': ''},\n",
        "    {'x': 359, 'y': 727, 'width': 27, 'height': 155, 'label': ''},\n",
        "    {'x': 164, 'y': 1044, 'width': 279, 'height': 163, 'label': ''}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "boxes = np.array([\n",
        "    [\n",
        "        box['x'],\n",
        "        box['y'],\n",
        "        box['x'] + box['width'],\n",
        "        box['y'] + box['height']\n",
        "    ] for box in boxes\n",
        "])"
      ],
      "metadata": {
        "id": "5rMQVWLKeutI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.set_image(image_rgb)\n",
        "\n",
        "masks, scores, logits = predictor.predict(\n",
        "    box=boxes,\n",
        "    multimask_output=False\n",
        ")\n",
        "\n",
        "# With one box as input, predictor returns masks of shape (1, H, W);\n",
        "# with N boxes, it returns (N, 1, H, W).\n",
        "if boxes.shape[0] != 1:\n",
        "    masks = np.squeeze(masks)\n"
      ],
      "metadata": {
        "id": "rbSrtSRIfRex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÁôåËÇø"
      ],
      "metadata": {
        "id": "Tee_x-u7r6h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Ëé∑ÂèñÂéüÂõæÂ∞∫ÂØ∏\n",
        "height, width = image_rgb.shape[:2]\n",
        "\n",
        "# Áªü‰∏ÄÂ§ÑÁêÜ mask Ê†ºÂºè\n",
        "if boxes.shape[0] == 1:\n",
        "    masks = masks[np.newaxis, :, :] if masks.ndim == 2 else masks\n",
        "else:\n",
        "    masks = np.squeeze(masks)  # (N, H, W)\n",
        "\n",
        "# ÊûÑÈÄ†‰øùÂ≠òË∑ØÂæÑ\n",
        "image_name = os.path.splitext(os.path.basename(IMAGE_PATH))[0]\n",
        "save_path = f\"{HOME}/{image_name}_label.png\"\n",
        "\n",
        "# Â¶ÇÊûúÊ†áÁ≠æÂõæÂ∑≤Â≠òÂú®ÔºåÂä†ËΩΩÂÆÉÔºõÂê¶ÂàôÂàõÂª∫Êñ∞Âõæ\n",
        "if os.path.exists(save_path):\n",
        "    label_map = np.array(Image.open(save_path))\n",
        "    print(f\"ËΩΩÂÖ•Â∑≤ÊúâÊ†áÁ≠æÂõæ: {save_path}\")\n",
        "else:\n",
        "    label_map = np.zeros((height, width), dtype=np.uint8)\n",
        "    print(f\"Êñ∞Âª∫Ê†áÁ≠æÂõæ: {save_path}\")\n",
        "\n",
        "# Â∞Ü mask Âå∫ÂüüËµãÂÄº‰∏∫ 1\n",
        "for mask in masks:\n",
        "    label_map[mask.astype(bool)] = 1\n",
        "\n",
        "# ‰øùÂ≠òÁªìÊûú\n",
        "Image.fromarray(label_map).save(save_path)\n",
        "print(f\"Ê†áÁ≠æÂõæÂ∑≤‰øùÂ≠òÂà∞: {save_path}\")\n"
      ],
      "metadata": {
        "id": "cfJV-rVJq60k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ê∑ãÂ∑¥Áªì"
      ],
      "metadata": {
        "id": "kBER4rzHtRxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Ëé∑ÂèñÂéüÂõæÂ∞∫ÂØ∏\n",
        "height, width = image_rgb.shape[:2]\n",
        "\n",
        "# Áªü‰∏ÄÂ§ÑÁêÜ mask Ê†ºÂºè\n",
        "if boxes.shape[0] == 1:\n",
        "    masks = masks[np.newaxis, :, :] if masks.ndim == 2 else masks\n",
        "else:\n",
        "    masks = np.squeeze(masks)  # (N, H, W)\n",
        "\n",
        "# ÊûÑÈÄ†‰øùÂ≠òË∑ØÂæÑ\n",
        "image_name = os.path.splitext(os.path.basename(IMAGE_PATH))[0]\n",
        "save_path = f\"{HOME}/{image_name}_label.png\"\n",
        "\n",
        "# Â¶ÇÊûúÊ†áÁ≠æÂõæÂ∑≤Â≠òÂú®ÔºåÂä†ËΩΩÂÆÉÔºõÂê¶ÂàôÂàõÂª∫Êñ∞Âõæ\n",
        "if os.path.exists(save_path):\n",
        "    label_map = np.array(Image.open(save_path))\n",
        "    print(f\"ËΩΩÂÖ•Â∑≤ÊúâÊ†áÁ≠æÂõæ: {save_path}\")\n",
        "else:\n",
        "    label_map = np.zeros((height, width), dtype=np.uint8)\n",
        "    print(f\"Êñ∞Âª∫Ê†áÁ≠æÂõæ: {save_path}\")\n",
        "\n",
        "# Â∞Ü mask Âå∫ÂüüËµãÂÄº‰∏∫ 2\n",
        "for mask in masks:\n",
        "    label_map[mask.astype(bool)] = 2\n",
        "\n",
        "# ‰øùÂ≠òÁªìÊûú\n",
        "Image.fromarray(label_map).save(save_path)\n",
        "print(f\"Ê†áÁ≠æÂõæÂ∑≤‰øùÂ≠òÂà∞: {save_path}\")"
      ],
      "metadata": {
        "id": "y4UxjvHmtVNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results visualisation"
      ],
      "metadata": {
        "id": "yL_HThW6fomp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections(\n",
        "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "    mask=masks.astype(bool)\n",
        ")\n",
        "\n",
        "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[source_image, segmented_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ],
      "metadata": {
        "id": "Gb9Retgzfj4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Â∞ÜÂèØËßÜÂåñÁöÑÂõæÂÉè‰øùÂ≠ò"
      ],
      "metadata": {
        "id": "kIAD7uthwtob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ÂàõÂª∫‰øùÂ≠òË∑ØÂæÑ\n",
        "output_dir = os.path.join(HOME, \"output\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Ëé∑ÂèñÂéüÂõæÂÉèÊñá‰ª∂ÂêçÔºà‰∏çÂ∏¶Ë∑ØÂæÑÔºâ\n",
        "image_name = os.path.basename(IMAGE_PATH)\n",
        "\n",
        "# ÊûÑÈÄ†‰øùÂ≠òË∑ØÂæÑ\n",
        "output_path = os.path.join(output_dir, image_name)\n",
        "\n",
        "# ‰øùÂ≠òÂàÜÂâ≤ÁªìÊûúÂõæÂÉè\n",
        "cv2.imwrite(output_path, segmented_image)\n",
        "print(f\"Segmented image saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "YcAiC4nVxpsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting with points"
      ],
      "metadata": {
        "id": "K2YhLwHiwOnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Execute cell below and use your mouse to **draw points** on the image üëá"
      ],
      "metadata": {
        "id": "ex_EELQswkZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "LvhZtqzcv1ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widget.bboxes"
      ],
      "metadata": {
        "id": "biiUhip93tol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 330, 'y': 450, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 191, 'y': 665, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 86, 'y': 879, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 425, 'y': 727, 'width': 0, 'height': 0, 'label': ''}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "input_point = np.array([\n",
        "    [\n",
        "        box['x'],\n",
        "        box['y']\n",
        "    ] for box in boxes\n",
        "])\n",
        "input_label = np.ones(input_point.shape[0])"
      ],
      "metadata": {
        "id": "xBc0Y_T139lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        ")"
      ],
      "metadata": {
        "id": "c6rS9seW4bbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ÂÅáËÆæ‰Ω†Êúâ masksÔºàÊé©ËÜúÔºâÂíå scoresÔºàÂæóÂàÜÔºâ\n",
        "# masks: ÂΩ¢Áä∂‰∏∫ (num_masks, H, W)\n",
        "# scores: ÂΩ¢Áä∂‰∏∫ (num_masks, )\n",
        "\n",
        "# ÂÅáËÆæ‰Ω†ÊúâÂéüÂõæ image_bgr\n",
        "\n",
        "alpha = 0.3  # ÈÄèÊòéÂ∫¶Ôºà0=ÂÆåÂÖ®ÈÄèÊòéÔºå1=ÂÆåÂÖ®‰∏çÈÄèÊòéÔºâ\n",
        "\n",
        "# Êé©ËÜúÂè†Âä†ÂáΩÊï∞ÔºöÂ∞ÜÂçï‰∏™Êé©ËÜúÁî® alpha blending ÁöÑÊñπÂºèÂè†Âä†Âà∞ÂõæÂÉè‰∏ä\n",
        "def overlay_mask_on_image(image, mask, color=(255, 0, 0), alpha=0.3):\n",
        "    # ËΩ¨‰∏∫ float32 ‰ª•‰æøÂä†ÊùÉÊ∑∑Âêà\n",
        "    image = image.astype(np.float32)\n",
        "    color_layer = np.full_like(image, color, dtype=np.float32)\n",
        "\n",
        "    # mask Êâ©Â±ï‰∏∫ 3 ÈÄöÈÅì\n",
        "    mask_3d = np.stack([mask] * 3, axis=-1).astype(np.float32)\n",
        "\n",
        "    # Ê∑∑ÂêàÂÖ¨Âºè\n",
        "    blended = image * (1 - mask_3d * alpha) + color_layer * (mask_3d * alpha)\n",
        "\n",
        "    return blended.astype(np.uint8)\n",
        "\n",
        "# ÂèØËßÜÂåñÂáΩÊï∞ÔºöÂØπÊØè‰∏™Êé©ËÜúËøõË°åÂè†Âä†Âπ∂ËøîÂõûÁªìÊûúÂõæÂÉè\n",
        "def visualize_masks_on_image(image_bgr, masks, scores):\n",
        "    num_masks = masks.shape[0]\n",
        "    images_with_masks = []\n",
        "\n",
        "    for i in range(num_masks):\n",
        "        mask = masks[i].astype(np.uint8)\n",
        "        blended = overlay_mask_on_image(image_bgr.copy(), mask, color=(255, 0, 0), alpha=alpha)\n",
        "        images_with_masks.append(blended)\n",
        "\n",
        "    return images_with_masks\n",
        "\n",
        "# Ëé∑ÂèñÂè†Âä†Êé©ËÜúÁöÑÂõæÂÉè\n",
        "images_with_masks = visualize_masks_on_image(image_bgr, masks, scores)\n",
        "\n",
        "# ÊòæÁ§∫ÂõæÂÉèÁΩëÊ†º\n",
        "sv.plot_images_grid(\n",
        "    images=images_with_masks,\n",
        "    titles=[f\"score: {score:.2f}\" for score in scores],\n",
        "    grid_size=(1, len(images_with_masks)),\n",
        "    size=(12, 12)\n",
        ")\n"
      ],
      "metadata": {
        "id": "d8Tz8igP2sjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    titles=[f\"score: {score:.2f}\" for score in scores],\n",
        "    grid_size=(1, 3),\n",
        "    size=(12, 12)\n",
        ")"
      ],
      "metadata": {
        "id": "A5tueOO05MwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}