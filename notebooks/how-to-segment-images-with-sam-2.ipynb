{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Segment Images with Segment Anything 2 (SAM2)\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything-2)\n",
        "\n",
        "Segment Anything Model 2 (SAM 2) is a foundation model designed to address promptable visual segmentation in both images and videos. The model extends its functionality to video by treating images as single-frame videos. Its design, a simple transformer architecture with streaming memory, enables real-time video processing. A model-in-the-loop data engine, which enhances the model and data through user interaction, was built to collect the SA-V dataset, the largest video segmentation dataset to date. SAM 2, trained on this extensive dataset, delivers robust performance across diverse tasks and visual domains.\n",
        "\n",
        "![segment anything model](https://media.roboflow.com/notebooks/examples/segment-anything-model-2-paper.jpg)\n",
        "\n",
        "This notebook is an extension of the official [notebook](https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb) prepared by Meta AI.\n",
        "\n",
        "## Complementary materials\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb)\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-segment-anything-2)\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on Segment Anything Model 2 (SAM2).\n",
        "\n",
        "[![SAM2 blogpost](https://media.roboflow.com/notebooks/examples/blog-what-is-sam-2.png)](https://blog.roboflow.com/what-is-segment-anything-2)"
      ],
      "metadata": {
        "id": "l2Es_L1iNX4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "8wkV75Db9tXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "DSffnnWDNhb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¸‹é¢è¿™æ®µä»£ç éå¿…é¡»è¿è¡Œï¼Œè¿˜æœ‰é™é¢å°±è¿è¡Œï¼Œæ²¡æœ‰é™é¢äº†å°±ä¸è¿è¡Œ"
      ],
      "metadata": {
        "id": "ESVebqHUxibk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA9bQMozM_wg",
        "outputId": "255d105c-ad72-4b15-ad44-83537a4a70ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ],
      "metadata": {
        "id": "H7YQbFlINnGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = '/content'\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx8UNmABNkJP",
        "outputId": "a5f6bf21-5930-4b69-a9d2-4fbeb5699dbb"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install SAM2 and dependencies"
      ],
      "metadata": {
        "id": "yo5LAKqyNzfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q\n",
        "!pip install -q supervision jupyter_bbox_widget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBrYxp6nNpqk",
        "outputId": "8657ae87-cb13-408e-9508-67cba22fba26"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'segment-anything-2'...\n",
            "remote: Enumerating objects: 1070, done.\u001b[K\n",
            "remote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1070/1070), 128.11 MiB | 26.46 MiB/s, done.\n",
            "Resolving deltas: 100% (381/381), done.\n",
            "/content/segment-anything-2/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download SAM2 checkpoints\n",
        "\n",
        "**NOTE:** SAM2 is available in 4 different model sizes ranging from the lightweight \"sam2_hiera_tiny\" (38.9M parameters) to the more powerful \"sam2_hiera_large\" (224.4M parameters)."
      ],
      "metadata": {
        "id": "g3Psmg3sOzIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints"
      ],
      "metadata": {
        "id": "Dq_DR0IJN_1H"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "BHRLQPV4WKd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
      ],
      "metadata": {
        "id": "vIcNq3IiXufS"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance.\n",
        "\n",
        "ä¸‹é¢è¿™æ®µä»£ç éå¿…é¡»è¿è¡Œï¼Œè¿˜æœ‰é™é¢å°±è¿è¡Œï¼Œæ²¡æœ‰é™é¢äº†å°±ä¸è¿è¡Œ"
      ],
      "metadata": {
        "id": "svThmVIGZZAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "GQpZNQPPxQtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLeXwS2UQU5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)"
      ],
      "metadata": {
        "id": "xHvgsf08QRZo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_generator = SAM2AutomaticMaskGenerator(sam2_model)"
      ],
      "metadata": {
        "id": "72oiBQYvUSws"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** OpenCV loads images in BGR format by default, so we convert to RGB for compatibility with the mask generator."
      ],
      "metadata": {
        "id": "wHrJV4HmavcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting with boxes"
      ],
      "metadata": {
        "id": "ijdVA3p0cyJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = SAM2ImagePredictor(sam2_model)"
      ],
      "metadata": {
        "id": "gZdNS8fJZ1B4"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE_PATHå¡«çš„æ˜¯ç¬¬ä¸€å¼ å›¾åƒçš„åœ°å€ï¼ŒæŠŠå›¾åƒæ”¾å“ªé‡Œäº†å°±å†™å“ªé‡Œã€‚\n",
        "é¦–æ¬¡è¿è¡Œè¯·è¿è¡Œè¿™ä¸€å•å…ƒæ ¼ï¼Œéé¦–æ¬¡è¿è¡Œï¼Œå³ç¬¬ä¸€å¼ ç…§ç‰‡å·²ç»æ ‡æ³¨å¥½äº†ï¼Œè¯·è·³è¿‡æœ¬å•å…ƒæ ¼ï¼Œè¿è¡Œâ€œåˆ‡æ¢ä¸‹ä¸€å¼ å›¾ç‰‡â€å•å…ƒæ ¼çš„å†…å®¹ã€‚"
      ],
      "metadata": {
        "id": "QuU33CNbxvoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = f\"{HOME}/MR/MR.jpg\"\n",
        "\n",
        "image_bgr = cv2.imread(IMAGE_PATH)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "HZTCMn0MeO7Q"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æ ‡æ³¨å®Œç¬¬ä¸€å¼ å›¾ç‰‡ä»¥åï¼Œåªä»è¿™é‡Œå¾€ä¸‹è¿è¡Œå°±å¯ä»¥äº†ï¼Œä¸Šé¢çš„å…¨éƒ¨éƒ½ä¸éœ€è¦è¿è¡Œäº†ï¼**"
      ],
      "metadata": {
        "id": "xnn1NbOT1PEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "åˆ‡æ¢ä¸‹ä¸€å¼ å›¾ç‰‡ã€‚folder_pathæ˜¯æ–‡ä»¶å¤¹æ‰€åœ¨åœ°å€ï¼Œæ”¹æˆè‡ªå·±çš„å›¾åƒæ‰€åœ¨æ–‡ä»¶å¤¹åœ°å€ã€‚\n",
        "æŠ¥â€œNo more imagesâ€è¯´æ˜è¯¥æ–‡ä»¶å¤¹é‡Œçš„æ‰€æœ‰å›¾åƒéå†ä¸€éäº†ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "AfFtAGHP4RUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "folder_path = f\"{HOME}/data\"\n",
        "\n",
        "current_image_name = os.path.basename(IMAGE_PATH)\n",
        "\n",
        "# è·å–æ’åºåçš„å›¾åƒåˆ—è¡¨\n",
        "image_list = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "# è·å–å½“å‰å›¾åƒç´¢å¼•å¹¶æ‰¾ä¸‹ä¸€ä¸ªå›¾åƒ\n",
        "current_index = image_list.index(current_image_name)\n",
        "if current_index + 1 < len(image_list):\n",
        "    next_image_name = image_list[current_index + 1]\n",
        "    IMAGE_PATH = os.path.join(folder_path, next_image_name)\n",
        "else:\n",
        "    print(\"No more images.\")\n",
        "    IMAGE_PATH = None\n",
        "\n",
        "# è¯»å–å¹¶å¤„ç†å›¾åƒ\n",
        "if IMAGE_PATH:\n",
        "    image_bgr = cv2.imread(IMAGE_PATH)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "print(IMAGE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "r8WA-OQgwBpS",
        "outputId": "cfae9d0a-9d59-4f2c-8461-90f9b1fc383e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "'MR.jpg' is not in list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-c7e5ca0c1846>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# è·å–å½“å‰å›¾åƒç´¢å¼•å¹¶æ‰¾ä¸‹ä¸€ä¸ªå›¾åƒ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcurrent_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_image_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcurrent_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnext_image_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'MR.jpg' is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interactive box prompt"
      ],
      "metadata": {
        "id": "MMzeqSsgejYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded"
      ],
      "metadata": {
        "id": "VNWUblWreTW6"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Execute cell below and use your mouse to **draw bounding box** on the image ğŸ‘‡"
      ],
      "metadata": {
        "id": "A9skH4bceoWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "gNwusrHEek_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widget.bboxes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SFmFWugeqR4",
        "outputId": "a0e442db-219d-4266-93f5-d888b1b4a1d9"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'x': 64, 'y': 139, 'width': 27, 'height': 24, 'label': ''}]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** `Sam2ImagePredictor.predict` method takes `np.ndarray` `box` argument in `[x_min, y_min, x_max, y_max]` format."
      ],
      "metadata": {
        "id": "G4vNm8trfN10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 166, 'y': 835, 'width': 99, 'height': 175, 'label': ''},\n",
        "    {'x': 472, 'y': 885, 'width': 168, 'height': 249, 'label': ''},\n",
        "    {'x': 359, 'y': 727, 'width': 27, 'height': 155, 'label': ''},\n",
        "    {'x': 164, 'y': 1044, 'width': 279, 'height': 163, 'label': ''}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "boxes = np.array([\n",
        "    [\n",
        "        box['x'],\n",
        "        box['y'],\n",
        "        box['x'] + box['width'],\n",
        "        box['y'] + box['height']\n",
        "    ] for box in boxes\n",
        "])"
      ],
      "metadata": {
        "id": "5rMQVWLKeutI"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.set_image(image_rgb)\n",
        "\n",
        "masks, scores, logits = predictor.predict(\n",
        "    box=boxes,\n",
        "    multimask_output=False\n",
        ")\n",
        "\n",
        "# With one box as input, predictor returns masks of shape (1, H, W);\n",
        "# with N boxes, it returns (N, 1, H, W).\n",
        "if boxes.shape[0] != 1:\n",
        "    masks = np.squeeze(masks)\n"
      ],
      "metadata": {
        "id": "rbSrtSRIfRex"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ï¼æ ‡æ³¨çš„æ˜¯ç™Œè‚¿è¿è¡Œæ­¤å•å…ƒæ ¼ã€‚"
      ],
      "metadata": {
        "id": "Tee_x-u7r6h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# è·å–åŸå›¾å°ºå¯¸\n",
        "height, width = image_rgb.shape[:2]\n",
        "\n",
        "# ç»Ÿä¸€å¤„ç† mask æ ¼å¼\n",
        "if boxes.shape[0] == 1:\n",
        "    masks = masks[np.newaxis, :, :] if masks.ndim == 2 else masks\n",
        "else:\n",
        "    masks = np.squeeze(masks)  # (N, H, W)\n",
        "\n",
        "label_dir = os.path.join(HOME, \"label\")\n",
        "os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# æ„é€ ä¿å­˜è·¯å¾„\n",
        "image_name = os.path.splitext(os.path.basename(IMAGE_PATH))[0]\n",
        "save_path = os.path.join(label_dir, f\"{image_name}_label.png\")\n",
        "\n",
        "\n",
        "# å¦‚æœæ ‡ç­¾å›¾å·²å­˜åœ¨ï¼ŒåŠ è½½å®ƒï¼›å¦åˆ™åˆ›å»ºæ–°å›¾\n",
        "if os.path.exists(save_path):\n",
        "    label_map = np.array(Image.open(save_path))\n",
        "    print(f\"è½½å…¥å·²æœ‰æ ‡ç­¾å›¾: {save_path}\")\n",
        "else:\n",
        "    label_map = np.zeros((height, width), dtype=np.uint8)\n",
        "    print(f\"æ–°å»ºæ ‡ç­¾å›¾: {save_path}\")\n",
        "\n",
        "# å°† mask åŒºåŸŸèµ‹å€¼ä¸º 1\n",
        "for mask in masks:\n",
        "    label_map[mask.astype(bool)] = 1\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "Image.fromarray(label_map).save(save_path)\n",
        "print(f\"æ ‡ç­¾å›¾å·²ä¿å­˜åˆ°: {save_path}\")\n"
      ],
      "metadata": {
        "id": "cfJV-rVJq60k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ï¼æ ‡æ³¨çš„æ˜¯æ·‹å·´ç»“è¿è¡Œæ­¤å•å…ƒæ ¼ã€‚"
      ],
      "metadata": {
        "id": "kBER4rzHtRxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# è·å–åŸå›¾å°ºå¯¸\n",
        "height, width = image_rgb.shape[:2]\n",
        "\n",
        "# ç»Ÿä¸€å¤„ç† mask æ ¼å¼\n",
        "if boxes.shape[0] == 1:\n",
        "    masks = masks[np.newaxis, :, :] if masks.ndim == 2 else masks\n",
        "else:\n",
        "    masks = np.squeeze(masks)  # (N, H, W)\n",
        "\n",
        "label_dir = os.path.join(HOME, \"label\")\n",
        "os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "# æ„é€ ä¿å­˜è·¯å¾„\n",
        "image_name = os.path.splitext(os.path.basename(IMAGE_PATH))[0]\n",
        "save_path = f\"{HOME}/{image_name}_label.png\"\n",
        "\n",
        "# å¦‚æœæ ‡ç­¾å›¾å·²å­˜åœ¨ï¼ŒåŠ è½½å®ƒï¼›å¦åˆ™åˆ›å»ºæ–°å›¾\n",
        "if os.path.exists(save_path):\n",
        "    label_map = np.array(Image.open(save_path))\n",
        "    print(f\"è½½å…¥å·²æœ‰æ ‡ç­¾å›¾: {save_path}\")\n",
        "else:\n",
        "    label_map = np.zeros((height, width), dtype=np.uint8)\n",
        "    print(f\"æ–°å»ºæ ‡ç­¾å›¾: {save_path}\")\n",
        "\n",
        "# å°† mask åŒºåŸŸèµ‹å€¼ä¸º 2\n",
        "for mask in masks:\n",
        "    label_map[mask.astype(bool)] = 2\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "Image.fromarray(label_map).save(save_path)\n",
        "print(f\"æ ‡ç­¾å›¾å·²ä¿å­˜åˆ°: {save_path}\")"
      ],
      "metadata": {
        "id": "y4UxjvHmtVNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results visualisation"
      ],
      "metadata": {
        "id": "yL_HThW6fomp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "detections = sv.Detections(\n",
        "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "    mask=masks.astype(bool)\n",
        ")\n",
        "\n",
        "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=[source_image, segmented_image],\n",
        "    grid_size=(1, 2),\n",
        "    titles=['source image', 'segmented image']\n",
        ")"
      ],
      "metadata": {
        "id": "Gb9Retgzfj4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å°†å¯è§†åŒ–çš„å›¾åƒä¿å­˜"
      ],
      "metadata": {
        "id": "kIAD7uthwtob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# åˆ›å»ºä¿å­˜è·¯å¾„\n",
        "output_dir = os.path.join(HOME, \"output\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# è·å–åŸå›¾åƒæ–‡ä»¶åï¼ˆä¸å¸¦è·¯å¾„ï¼‰\n",
        "image_name = os.path.basename(IMAGE_PATH)\n",
        "\n",
        "# æ„é€ ä¿å­˜è·¯å¾„\n",
        "output_path = os.path.join(output_dir, image_name)\n",
        "\n",
        "# ä¿å­˜åˆ†å‰²ç»“æœå›¾åƒ\n",
        "cv2.imwrite(output_path, segmented_image)\n",
        "print(f\"Segmented image saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "YcAiC4nVxpsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting with points"
      ],
      "metadata": {
        "id": "K2YhLwHiwOnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Execute cell below and use your mouse to **draw points** on the image ğŸ‘‡"
      ],
      "metadata": {
        "id": "ex_EELQswkZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "\n",
        "widget = BBoxWidget()\n",
        "widget.image = encode_image(IMAGE_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "LvhZtqzcv1ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widget.bboxes"
      ],
      "metadata": {
        "id": "biiUhip93tol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 330, 'y': 450, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 191, 'y': 665, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 86, 'y': 879, 'width': 0, 'height': 0, 'label': ''},\n",
        "    {'x': 425, 'y': 727, 'width': 0, 'height': 0, 'label': ''}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "input_point = np.array([\n",
        "    [\n",
        "        box['x'],\n",
        "        box['y']\n",
        "    ] for box in boxes\n",
        "])\n",
        "input_label = np.ones(input_point.shape[0])"
      ],
      "metadata": {
        "id": "xBc0Y_T139lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        ")"
      ],
      "metadata": {
        "id": "c6rS9seW4bbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# å‡è®¾ä½ æœ‰ masksï¼ˆæ©è†œï¼‰å’Œ scoresï¼ˆå¾—åˆ†ï¼‰\n",
        "# masks: å½¢çŠ¶ä¸º (num_masks, H, W)\n",
        "# scores: å½¢çŠ¶ä¸º (num_masks, )\n",
        "\n",
        "# å‡è®¾ä½ æœ‰åŸå›¾ image_bgr\n",
        "\n",
        "alpha = 0.3  # é€æ˜åº¦ï¼ˆ0=å®Œå…¨é€æ˜ï¼Œ1=å®Œå…¨ä¸é€æ˜ï¼‰\n",
        "\n",
        "# æ©è†œå åŠ å‡½æ•°ï¼šå°†å•ä¸ªæ©è†œç”¨ alpha blending çš„æ–¹å¼å åŠ åˆ°å›¾åƒä¸Š\n",
        "def overlay_mask_on_image(image, mask, color=(255, 0, 0), alpha=0.3):\n",
        "    # è½¬ä¸º float32 ä»¥ä¾¿åŠ æƒæ··åˆ\n",
        "    image = image.astype(np.float32)\n",
        "    color_layer = np.full_like(image, color, dtype=np.float32)\n",
        "\n",
        "    # mask æ‰©å±•ä¸º 3 é€šé“\n",
        "    mask_3d = np.stack([mask] * 3, axis=-1).astype(np.float32)\n",
        "\n",
        "    # æ··åˆå…¬å¼\n",
        "    blended = image * (1 - mask_3d * alpha) + color_layer * (mask_3d * alpha)\n",
        "\n",
        "    return blended.astype(np.uint8)\n",
        "\n",
        "# å¯è§†åŒ–å‡½æ•°ï¼šå¯¹æ¯ä¸ªæ©è†œè¿›è¡Œå åŠ å¹¶è¿”å›ç»“æœå›¾åƒ\n",
        "def visualize_masks_on_image(image_bgr, masks, scores):\n",
        "    num_masks = masks.shape[0]\n",
        "    images_with_masks = []\n",
        "\n",
        "    for i in range(num_masks):\n",
        "        mask = masks[i].astype(np.uint8)\n",
        "        blended = overlay_mask_on_image(image_bgr.copy(), mask, color=(255, 0, 0), alpha=alpha)\n",
        "        images_with_masks.append(blended)\n",
        "\n",
        "    return images_with_masks\n",
        "\n",
        "# è·å–å åŠ æ©è†œçš„å›¾åƒ\n",
        "images_with_masks = visualize_masks_on_image(image_bgr, masks, scores)\n",
        "\n",
        "# æ˜¾ç¤ºå›¾åƒç½‘æ ¼\n",
        "sv.plot_images_grid(\n",
        "    images=images_with_masks,\n",
        "    titles=[f\"score: {score:.2f}\" for score in scores],\n",
        "    grid_size=(1, len(images_with_masks)),\n",
        "    size=(12, 12)\n",
        ")\n"
      ],
      "metadata": {
        "id": "d8Tz8igP2sjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.plot_images_grid(\n",
        "    images=masks,\n",
        "    titles=[f\"score: {score:.2f}\" for score in scores],\n",
        "    grid_size=(1, 3),\n",
        "    size=(12, 12)\n",
        ")"
      ],
      "metadata": {
        "id": "A5tueOO05MwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}